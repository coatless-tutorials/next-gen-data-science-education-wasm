---
title: "Demo: Data Science Education with WebAssembly"
subtitle: "Linear Regression in R and Python"
format: 
    live-revealjs: 
        scrollable: true
webr:
    packages: 
        - ggplot2
pyodide: 
    packages: 
        - scikit-learn
        - pandas
        - matplotlib
        - seaborn
        - statsmodels

engine: knitr
---

{{< include ../_extensions/r-wasm/live/_knitr.qmd >}}

## Overview

The goal of this presentation is to showcase the power of WebAssembly (WASM) in data science education by allowing real-time code execution, visualization, and exercises directly within the slide deck.

We do this by exploring the concept of linear regression using both R and Python code snippets.


---

## Introduction

Linear regression is a fundamental statistical technique used to model the relationship between a dependent variable and one or more independent variables.

This presentation will cover:

1. Basic Concepts
2. Implementation in R and Python
3. Model Evaluation
4. Assumptions and Diagnostics

---

## Basic Concepts

Linear regression aims to find the best-fitting straight line through the data points.

The general form of a simple linear regression model is:

$$Y = \beta_0 + \beta_1X + \epsilon$$

Where:

- $Y$ is the dependent variable
- $X$ is the independent variable
- $\beta_0$ is the y-intercept
- $\beta_1$ is the slope
- $\epsilon$ is the error term

---

## Generating Data

Let's look at how to implement linear regression in R and Python by first simulating some data

::: {.panel-tabset group="language"}

## R

```{webr}
# Create sample data
set.seed(123)
x <- 1:100
y <- 2 * x + 1 + rnorm(100, mean = 0, sd = 3)
df <- data.frame(x = x, y = y)

head(df)
```

## Python

```{pyodide}
import numpy as np
import pandas as pd

# Create sample data
np.random.seed(123)
x = np.arange(1, 101)
y = 2 * x + 1 + np.random.normal(0, 3, 100)
data = pd.DataFrame({'x': x, 'y': y})

data.head()
```

:::

---

## Guessing the Coefficients

Try to fit a linear regression model by hand through manipulating coefficients below:

The linear regression with $\beta_0 =$
`{ojs} beta_0_Tgl` and $\beta_1 =$ `{ojs} beta_1_Tgl` is:

```{ojs}
//| echo: false
import {Tangle} from "@mbostock/tangle"

// Setup Tangle reactive inputs
viewof beta_0 = Inputs.input(0);
viewof beta_1 = Inputs.input(1);
beta_0_Tgl = Inputs.bind(Tangle({min: -30, max: 300, minWidth: "1em", step: 1}), viewof beta_0);
beta_1_Tgl = Inputs.bind(Tangle({min: -5, max: 5, minWidth: "1em", step: 0.25}), viewof beta_1);

// draw plot in R
regression_plot(beta_0, beta_1)
```

```{webr}
#| edit: false
#| output: false
#| define:
#|   - regression_plot
regression_plot <- function(beta_0, beta_1) {

  # Create scatter plot  
  plot(
    df$x, df$y, 
    xlim = c(min(df$x) - 10, max(df$x) + 10),
    ylim = c(min(df$y) - 10, max(df$y) + 10)
  )

  # Graph regression line
  abline(a = beta_0, b = beta_1, col = "red")
}
```
:::


---

## Fit Linear Regression Model

Now that we have our data, let's fit a linear regression model to it:

::: {.panel-tabset group="language"}

## R

```{webr}
# Fit linear regression model
model <- lm(y ~ x, data = df)

# View summary of the model
summary(model)
```

## Python

```{pyodide}
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Fit linear regression model
model = LinearRegression()
model.fit(data[['x']], data['y'])

# Print model coefficients
print(f"Intercept: {model.intercept_:.2f}")
print(f"Slope: {model.coef_[0]:.2f}")
```

:::

## Visualize the Results

We can visualize the data and the regression line to see how well the model fits the data using ggplot2 in R and Matplotlib in Python.

::: {.panel-tabset group="language"}

## R

```{webr}
library(ggplot2) 

# Plot the data and regression line
ggplot(df, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  theme_minimal() +
  labs(title = "Linear Regression in R",
       x = "X", y = "Y")
```

## Python

```{pyodide}
# Plot the data and regression line
plt.figure(figsize=(10, 6))
plt.scatter(data['x'], data['y'])
plt.plot(data['x'], model.predict(data[['x']]), color='red')
plt.title("Linear Regression in Python")
plt.xlabel("X")
plt.ylabel("Y")
plt.show()
```

:::

---

## Predicting New Values

We can use our linear regression model to make predictions on new data:

::: {.panel-tabset group="language"}

## R

```{webr}
# Predict new values
new_data <- data.frame(x = c(101, 102, 103))
predictions <- predict(model, newdata = new_data)

predictions
```

## Python

```{pyodide}
# Predict new values
new_data = pd.DataFrame({'x': [101, 102, 103]})
predictions = model.predict(new_data)
```

:::

---

## Your Turn: Predict New Values!

{{< countdown "01:30" top="10px" right="5px">}}

Create a new data frame with `x` values 10, 30, and 60, then use the model to predict the corresponding y values.

::: {.panel-tabset group="language"}

## R 

```{webr}
#| exercise: ex_1_r
# Create your new data frame here
_______

# Make predictions here
_______

# Print the predictions
_______
```

```{webr}
#| exercise: ex_1_r
#| check: true

# Create your new data frame here
new_data <- data.frame(x = c(10, 30, 60))

# Make predictions here
predictions <- predict(model, newdata = new_data)

if (isTRUE(all.equal(.result, predictions))) {
  list(correct = TRUE, message = "Nice work!")
} else {
  list(correct = FALSE, message = "That's incorrect, sorry.")
}
```

## Python 

```{pyodide}
#| exercise: ex_1_py
# Create your new Pandas data frame here
_______

# Make predictions using the model
_______

# Print the predictions
_______
```

```{pyodide}
#| exercise: ex_1_py
#| check: true

# Create a new DataFrame with x values 10, 30, and 60
new_data_solution = pd.DataFrame({'x': [10, 30, 60]})

# Make predictions using the model
predictions_solution = model.predict(new_data_solution)

feedback = None
if (result == predictions_solution):
  feedback = { "correct": True, "message": "Nice work!" }
else:
  feedback = { "correct": False, "message": "That's incorrect, sorry." }

feedback
```

:::


---

## Model Evaluation

We can evaluate the performance of our linear regression model using various metrics:

::: {.panel-tabset}

## R

```{webr}
# R-squared
summary(model)$r.squared

# Root Mean Squared Error (RMSE)
sqrt(mean(residuals(model)^2))

# Mean Absolute Error (MAE)
mean(abs(residuals(model)))
```

## Python

```{pyodide}
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# R-squared
r2 = r2_score(data['y'], model.predict(data[['x']]))

# Root Mean Squared Error (RMSE)
rmse = np.sqrt(mean_squared_error(data['y'], model.predict(data[['x']])))

# Mean Absolute Error (MAE)
mae = mean_absolute_error(data['y'], model.predict(data[['x']]))

print(f"R-squared: {r2:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"MAE: {mae:.4f}")
```

:::

---

## Assumptions 

Linear regression relies on several assumptions:

1. Linearity
2. Independence
3. Homoscedasticity
4. Normality of residuals

---

## Checking Assumptions with Diagnostics Plots

Let's look at some diagnostic plots:

::: {.panel-tabset}

## R

```{webr}
par(mfrow = c(2, 2))
plot(model)
```

## Python

```{pyodide}
import seaborn as sns

# Residual plot
plt.figure(figsize=(10, 6))
sns.residplot(x=model.predict(data[['x']]), y=data['y'], lowess=True)
plt.title("Residual Plot")
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.show()

# Q-Q plot
from scipy import stats

fig, ax = plt.subplots(figsize=(10, 6))
_, (__, ___, r) = stats.probplot(model.resid, plot=ax, fit=True)
ax.set_title("Q-Q Plot")
plt.show()
```

:::

---

## Conclusion

- Linear regression is a powerful tool for modeling relationships between variables.
- Both R and Python offer robust implementations and diagnostic tools.
- Always check assumptions and perform diagnostics to ensure the validity of your model.
- Consider more advanced techniques (e.g., multiple regression, polynomial regression) for complex relationships.